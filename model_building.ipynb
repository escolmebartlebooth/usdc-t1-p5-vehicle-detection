{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "This notebook takes the features extracted in feature_extraction.ipynb and uses them to train and test various models with various parameters to establish the 'best-fit' model that can then be saved and used for the prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 101: build a 'bog-standard' linear svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 17760 feature vectors with 8412 features per image\n"
     ]
    }
   ],
   "source": [
    "# let's load the pre-processed training data\n",
    "filename = 'data/training_data'\n",
    "temp = open(filename, 'r+b')\n",
    "try:\n",
    "    training_data = np.load(filename)\n",
    "    training_data.files\n",
    "finally:\n",
    "    temp.close()\n",
    "\n",
    "# some stats\n",
    "X = training_data['X']\n",
    "y = training_data['y']\n",
    "\n",
    "print('we have {} feature vectors with {} features per image'.format(len(X), len(X[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length: 8412\n",
      "Linear SVC...\n",
      "30.97 Seconds to train...\n",
      "Test Accuracy of model =  0.987\n",
      "0.08 Seconds to predict all test image vectors...\n",
      "SVC...\n",
      "268.09 Seconds to train...\n",
      "Test Accuracy of model =  0.9899\n",
      "64.9 Seconds to predict all test image vectors...\n",
      "Gaussian Bayes...\n",
      "2.68 Seconds to train...\n",
      "Test Accuracy of model =  0.9062\n",
      "1.01 Seconds to predict all test image vectors...\n",
      "Decision Tree...\n",
      "290.83 Seconds to train...\n",
      "Test Accuracy of model =  0.9642\n",
      "0.12 Seconds to predict all test image vectors...\n"
     ]
    }
   ],
   "source": [
    "# build a simple model\n",
    "def split_and_scale_data(X, y, test_size=0.2):\n",
    "    rand_state = np.random.randint(0, 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state)\n",
    "\n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    # Apply the scaler to X\n",
    "    X_train = X_scaler.transform(X_train)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "\n",
    "    print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_scaler\n",
    "\n",
    "def build_model(clf, X_train, y_train):\n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train...')\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def test_model(clf, X_test, y_test):\n",
    "    # Check the score of the model\n",
    "    t=time.time()\n",
    "    print('Test Accuracy of model = ', round(clf.score(X_test, y_test), 4))\n",
    "    # Check the prediction time for a single sample\n",
    "    t2=time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to predict all test image vectors...')\n",
    "    \n",
    "X_train, y_train, X_test, y_test, X_scaler = split_and_scale_data(X, y)\n",
    "\n",
    "# let's try bog standard svc, gaussian bayes, decision tree\n",
    "\n",
    "# Use a linear SVC \n",
    "print('Linear SVC...')\n",
    "lsvc = LinearSVC()\n",
    "lsvc = build_model(lsvc, X_train, y_train)\n",
    "test_model(lsvc, X_test, y_test)\n",
    "\n",
    "# Use a non-linear SVC \n",
    "print('SVC...')\n",
    "svc = svm.SVC(kernel='rbf')\n",
    "svc = build_model(svc, X_train, y_train)\n",
    "test_model(svc, X_test, y_test)\n",
    "\n",
    "# Gaussian Bayes\n",
    "print('Gaussian Bayes...')\n",
    "clf = GaussianNB()\n",
    "clf = build_model(clf, X_train, y_train)\n",
    "test_model(clf, X_test, y_test)\n",
    "\n",
    "# decision tree\n",
    "print('Decision Tree...')\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = build_model(clf, X_train, y_train)\n",
    "test_model(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important points here are that the feature vectors were shuffled into a training and test set at random (although the time series data in the training set calls for a more robust method of splitting). The training data was scaled using the standard scalar (zero mean and unit variance). The test data was then separately scaled using the fitted scalar.\n",
    "\n",
    "In addition, it's clear that out of the box, the linear svm has a really good initial testing accuracy and is relatively quick to train and very quick to predict. The rbf kernel for svc takes quite a lot longer to train but has even better initial accuracy but takes longer to predict all the test data. The Gaussian Bayes accuracy is lower, whilst the decision tree has good accuracy but takes a long time to train cf. svm.\n",
    "\n",
    "It's time to tune the models and create a pickle output for the best model so it can be loaded back in at a later date with the feature extraction parameters and scalar for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model tuning and pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_model(clf, scaler, params):\n",
    "    # create a dictionary to pickle\n",
    "    pickled_objects = {\n",
    "        'model': clf,\n",
    "        'scaler': scaler,\n",
    "        'params': params\n",
    "    }\n",
    "    \n",
    "    # create output file\n",
    "    filename = 'model.pkl'\n",
    "    output = open(filename, 'wb')\n",
    "\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(pickled_objects, output)\n",
    "\n",
    "    output.close()\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': None, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)}\n"
     ]
    }
   ],
   "source": [
    "# let's try a pickle of the rbf svc\n",
    "\n",
    "filename = pickle_model(lsvc, X_scaler, None)\n",
    "\n",
    "pkl_file = open(filename, 'rb')\n",
    "\n",
    "data1 = pickle.load(pkl_file)\n",
    "print(data1)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning models involve cycling through combinations of hyper-paramters for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the SVC - we can tune kernel and 'c' - will split into 2\n",
    "parameters = {'kernel':('sigmoid', 'rbf'), 'C':[1, 10], 'gamma':[0.0001, 0.005]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf = build_model(clf, X_train, y_train)\n",
    "print(sorted(clf.cv_results_.keys()))\n",
    "test_model(clf, X_test, y_test)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'poly'), 'C':[1, 10], 'gamma':[0.0001, 0.005]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf = build_model(clf, X_train, y_train)\n",
    "print(sorted(clf.cv_results_.keys()))\n",
    "test_model(clf, X_test, y_test)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the decision tree - we can tune criterion, max-depth and min samples split - will split into 3\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[4], 'min_samples_split':[2, 5, 10]}\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(tree, parameters)\n",
    "clf = build_model(clf, X_train, y_train)\n",
    "print(sorted(clf.cv_results_.keys()))\n",
    "test_model(clf, X_test, y_test)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the decision tree - we can tune criterion, max-depth and min samples split - will split into 3\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[8], 'min_samples_split':[2, 5, 10]}\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(tree, parameters)\n",
    "clf = build_model(clf, X_train, y_train)\n",
    "print(sorted(clf.cv_results_.keys()))\n",
    "test_model(clf, X_test, y_test)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the decision tree - we can tune criterion, max-depth and min samples split - will split into 3\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[10], 'min_samples_split':[2, 5, 10]}\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(tree, parameters)\n",
    "clf = build_model(clf, X_train, y_train)\n",
    "print(sorted(clf.cv_results_.keys()))\n",
    "test_model(clf, X_test, y_test)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
